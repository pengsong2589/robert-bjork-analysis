# W3基础作业：学者信息库整理

## 任务目标

- [x] 掌握对原始、混杂的学术信息进行结构化整理的四大核心步骤：编码、去重、分组、完善。
- [x] 训练运用AI工具或设计精妙提示词（Prompts）来自动化、半自动化地执行信息整理任务，提升效率与质量，以便更好地分析。
- [x] 学习应用信息编码/标签体系（Taxonomy）。  
- [x] 建立“脏数据不可用，好数据是资产”的专业信息分析意识。

## 结果
1. 工作量概述：本次作业，我整理了共计**30**条论文信息，**73**条推特数据。
2. 整理后的照片：

批量重命名的PDF文件
![](https://s21.ax1x.com/2025/07/27/pVYpAj1.png)

按年份整理的文献PDF
![](https://s21.ax1x.com/2025/07/27/pVYpF39.png)

重新整理的推特md文件，信息丰富
![](https://s21.ax1x.com/2025/07/27/pVYpkcR.png)
![](https://s21.ax1x.com/2025/07/27/pVYpeHK.png)

xlsx文件，包含机构信息、关键词等数据
![](https://s21.ax1x.com/2025/07/27/pVYpZB6.png)

## 过程

## 实操前的准备工作：
通过命令行```mkdir -p data/{raw,interim,processed} scripts && touch README.md ```创建文件夹和文件。

```
/Users/tommy/Projects/projects/AI学术分析/智能整理/
├── README.md                 # 项目说明文档
├── W3基础作业.md              # 基础作业文档
├── data/                     # 数据存储目录
│   ├── raw/                  # 原始数据（永不改动）
│   ├── interim/              # 中间处理文件（每一步生成新文件）
│   ├── processed/            # 最终清洗后的分析数据
│   └── backup/               # 重要文件备份目录
└── scripts/                  # 数据处理脚本
```


## ① 编码 (Encoding):

01 导出Zotero元数据：W2基础作业我选择了Trevor Robbins作为学者分析，但是经过分析发现他并没有太多社交媒体的信息，于是我重走一遍```智能获取```，选择了"*Geoffrey E. Hinton*"作为学者分析对象。获取了他的OpenAlex元数据268篇文献，导出为.RIS格式；接着获取他的Twitter数据73条（因为原本的账户试用结束，换了个账户继续试用）。

导出"*Geoffrey E. Hinton*"总引用数前30的论文元数据（.csv），命名为"202507271930_Hinton_Papers_30.csv"，查看DOI数据，无缺失。直接进入下一步，为PDF文件命名。

02 为PDF文件命名：使用```python scripts/202507271930_Hinton_Papers_30.py```脚本，将DOI作为PDF文件名，并添加到Zotero中。

提示词：
```
[请处理 @/Hinton_papers_PDF_30  中的所有论文PDF文件。根据 @202507271930_Hinton_Papers_30.csv  中提供的DOI信息，将每个PDF复制一份，并将副本重命名为以下格式：
[DOI].pdf（DOI使用方括号[]包裹，保留PDF后缀）  
**要求：**  
1. 确保每个PDF与对应的DOI正确匹配（基于@202507271930_Hinton_Papers_30.csv 提供的DOI元数据）。  
2. 保留原始文件，不覆盖、不删除。  
3. 若某篇论文在 202507271930_Hinton_Papers_30 中找不到DOI，请将该文件记录到一个清单（缺失清单），不要随意命名。]
```

> 发现PDF文件名并没有完全和DOI匹配，比如```[10.1001_jama.2018.11100].pdf```和CSV文件中DOI```10.1001/jama.2018.11100```不一致，于是修改脚本，将CSV文件中DOI中的`/`替换为`_`，再进行匹配。成功修改，并存储为新的CSV文件"202507271930_Hinton_Papers_30_v2.csv"。
验证：```现在来验证，CSV文件中的DOI和文件夹中PDF命名的DOI是否完全一致```，验证报告显示：```[验证结果：所有DOI均匹配成功，无遗漏。]```。

03 为补充类信息即Twitter数据设置唯一编码
通过apify导出的twitter数据"dataset_twitter-Hinton-tweets-scraper_2025-07-27.csv"，根据提示词：
```
请处理 @dataset_twitter-Hinton-tweets-scraper_2025-07-27.csv  文件，将其中的每条 tweet 转换为独立的markdown(.md) 文件，并生成可追溯的索引。
具体要求如下：
1. **Markdown 文件生成**
    - 每条记录生成一个`.md`文件，内容仅包含该记录的`full_text`列内容。
    - 每个 .md 文件使用created_at列的时间戳作为文件名，格式：YYYY-MM-DD-HH-MM-SS.md，例如 2025-07-11-14-15-34.md。
    - 确保文件名唯一，如遇重复时间戳，在文件名末尾追加序号 (如 -1, -2)。
2. **Excel 索引文件生成 (使用软链接)**
    - 在原始`CSV`基础上生成一个`Excel`文件`(.xlsx)`，保留`CSV`所有字段。
    - 新增一列`md_file`，记录该`tweet`对应 .md 文件的 相对路径软链接。
    - 建议 .md 文件和 tweets_with_md.xlsx 位于同一目录，以便 md_file 列可直接用文件名 (如 2025-07-11-14-15-34.md)，无需长路径。
    - 输出 Excel 文件命名为 tweets_with_md.xlsx。
3. **文件存储结构**
    - 新建目录 data/processed/tweets_md/，将所有 .md 文件及 tweets_with_md.xlsx 存放于该目录下。
    - 原始 dataset_twitter-Hinton-tweets-scraper_2025-07-27.csv 保持在 data/raw/ 目录，不得改动。
4. 最终输出结果需包括：
- 所有 .md 文件 (存于 data/processed/tweets_md/)
- tweets_with_md.xlsx (完整字段 + md_file 软链接列)
```
反思：
> 生成对应的30个.md文件，包含时间戳和text内容。md文件和Excel文件生成成功，但是Excel文件中的md_file没有形成软链接，故重新让AI生成Excel文件，并添加软链接。发现还是不行，手动查看，发现超链接地址设置的为"Users\tommy\Projects\projects\AI%E5%AD%A6%E6%9C%AF%E5%88%86%E6%9E%90\%E6%99%BA%E8%83%BD%E6%95%B4%E7%90%86\data\processed\tweets_md\2025-06-08-10-58-10.md"，projects\后面是乱码，所以链接失败。

> AI分析后，问题在于我使用了 **urllib.parse.quote()** 对整个文件路径进行了URL编码，导致中文字符（如"AI学术分析"、"智能整理"）被编码成了 %XX 形式，Excel无法正确识别这种编码后的本地文件路径。
让我分析并修复这个问题：
问题原因：
当前代码使用了 urllib.parse.quote(md_file_full_path)
路径中的中文字符被URL编码：
"AI学术分析" → "AI%E5%AD%A6%E6%9C%AF%E5%88%86%E6%9E%90"
"智能整理" → "%E6%99%BA%E8%83%BD%E6%95%B4%E7%90%86"
解决方案：
直接使用绝对路径+file://协议，不进行URL编码，让Excel能正确识别本地文件路径。```

问题解决，成功链接。


## ② 去重 (Deduplication):
接下来对"Geoffrey E. Hinton"的论文进行去重，首先使用Zotero自带的去重功能，查找是否存在重复，发现只有一条数据"Managing extreme AI risks amid rapid progress"，并且只有URL有所不同，但是条目没有重复。
接下来用py脚本来对zotero.sqlite数据库进行查重，为防止数据损坏，先备份zotero.sqlite，再运行脚本。脚本运行后，发现 一些重复条目，但是感觉不够完善，也未发现Geoffery Hinton的论文条目存在重复。
收到检索Hinton的tiwtter数据，发现没有重复，故跳过这一步。

## ③ 分组 (Grouping):
接下来对"Geoffery E. Hinton"的30篇PDF论文进行分组，分别尝试时间、空间、变量分组。

时间分组：按照论文发表时间，将论文分为不同年份的文件夹，并按照论文发表时间排序，将论文按照年份存入对应的文件夹中。

结果如下：
```
data/processed/organized_papers_by_year/
├── 1983/ (1篇) - Parallel visual computation
├── 1985/ (1篇) - A Learning Algorithm for Boltzmann Machines  
├── 1986/ (1篇) - Learning representations by back-propagating errors
├── 1989/ (2篇) - 神经网络时延处理研究
├── 1990/ (1篇) - 时延神经网络架构
├── 1991/ (1篇) - 专家混合模型
├── 1992/ (3篇) - 神经网络简化和经验学习
├── 1993/ (1篇) - 描述长度最小化
├── 1995/ (2篇) - Wake-Sleep算法和Helmholtz机器
├── 2002/ (1篇) - 贝叶斯推理理论
├── 2006/ (2篇) - 深度学习突破年：DBN和降维技术
├── 2007/ (1篇) - 受限玻尔兹曼机协同过滤
├── 2008/ (1篇) - 语义哈希
├── 2009/ (1篇) - 深度信念网络综述
├── 2011/ (1篇) - 深度信念网络声学建模
├── 2012/ (2篇) - 深度神经网络语音识别应用
├── 2013/ (4篇) - 📈 深度学习应用高峰年
├── 2014/ (1篇) - 自然语言理解应用
├── 2017/ (1篇) - ImageNet突破性成果
├── 2018/ (1篇) - 深度学习医疗应用
├── 2021/ (1篇) - 深度学习AI综述
└── unknown/ (0篇) - 空目录
```
| 年代区间 | 论文数量 | 占比 | 备注 |
|---------|---------|------|------|
| 1980s   | 4篇     | 13.3% | 神经网络基础理论发展期 |
| 1990s   | 8篇     | 26.7% | 反向传播算法成熟期 |
| 2000s   | 4篇     | 13.3% | 深度学习理论准备期 |
| 2010s   | 13篇    | 43.3% | 深度学习爆发期 |
| 2020s   | 1篇     | 3.3%  | 当前发展期 |

空间分组：按照论文发表的空间，将论文分为不同地点的文件夹，并按照论文发表时间排序，将论文按照地点存入对应的文件夹中。
输入课程提供的提示词，运行，发现问题。
> AI：看起来脚本运行完成了，但是所有的DOI查询都没有找到机构信息。这可能是因为CSV文件中的DOI格式（使用下划线```_```替代斜杠```/```）与OpenAlex API期望的格式不匹配。让我先查看生成的Excel文件，然后修改脚本来解决这个问题。
问题解决，成功运行。机构信息获取成功率93.3%，成功获取了28篇论文的机构信息。2篇没查到的，手动检索。
- **论文总数**: 30篇
- **成功查询**: 28篇
- **查询失败**: 2篇
- **整体成功率**: 93.3%

 🏫 Geoffrey Hinton 主要关联机构分析

| 排名 | 机构名称 | 论文数量 | 占比 | 备注 |
|------|----------|----------|------|------|
| 1 | University of Toronto | 22篇 | 73.3% | 主要工作机构 |
| 2 | Carnegie Mellon University | 3篇 | 10.0% | 早期工作机构 |
| 3 | Canadian Institute for Advanced Research | 2篇 | 6.7% | 长期合作机构 |
| 4 | University of New Brunswick | 2篇 | 6.7% | 加拿大合作机构 |
| 5 | Google | 1篇 | 3.3% | 工业界合作 |
| 6 | OpenAI | 1篇 | 3.3% | 最新合作 |


变量分组：按照论文的变量，将论文分为不同变量的文件夹，并按照论文发表时间排序，将论文按照变量存入对应的文件夹中。
匹配统计
- **DOI匹配成功**: 20个条目
- **标题匹配成功**: 0个条目
- **未匹配**: 10个条目
- **总计**: 30个条目

补充: 对于未匹配的10个条目，我让AI编写脚本利用markdownify-mcp阅读10条PDF论文，并提取论文的标题和摘要，然后使用标题和摘要进行匹配。成功运行，详细如下：
```
成功处理的10个PDF文件：
1. **Deep learning for AI (2021)**
   - PDF: `[10.1145_3448250].pdf`
   - 提取主题: `Neural Network; Deep Learning`

2. **How Neural Networks Learn from Experience (1992)**
   - PDF: `[10.1038_scientificamerican0992-144].pdf`
   - 提取主题: `Neural Network`

3. **A time-delay neural network architecture for isolated word recognition (1990)**
   - PDF: `[10.1016_0893-6080(90)90044-l].pdf`
   - 提取主题: `Neural Network; Hidden Markov; Mutual Information; Markov; Speech Recognition`

4. **Deep Learning—A Technology With the Potential to Transform Health Care (2018)**
   - PDF: `[10.1001_jama.2018.11100].pdf`
   - 提取主题: `Neural Network; Deep Learning; Classification; Convolutional; Regression; Artificial Intelligence; Speech Recognition`

5. **Simplifying Neural Networks by Soft Weight-Sharing (1992)**
   - PDF: `[10.1162_neco.1992.4.4.473].pdf`
   - 提取主题: `Neural Network; Clustering`

6. **Classical and Bayesian Inference in Neuroimaging: Theory (2002)**
   - PDF: `[10.1006_nimg.2002.1090].pdf`
   - 提取主题: `Bayesian`

7. **Deep belief networks (2009)**
   - PDF: `[10.4249_scholarpedia.5947].pdf`
   - 提取主题: `Variational; Boltzmann Machine; Backpropagation; Restricted Boltzmann Machine; Deep Belief Network; Autoencoder`

8. **Keeping the neural networks simple by minimizing the description length of the weights (1993)**
   - PDF: `[10.1145_168304.168306].pdf`
   - 提取主题: `Neural Network`

9. **New types of deep neural network learning for speech recognition and related applications: an overview (2013)**
   - PDF: `[10.1109_icassp.2013.6639344].pdf`
   - 提取主题: `Neural Network; Deep Learning; Recurrent; Optimization; Speech Recognition; Machine Learning`

10. **Semantic hashing (2008)**
    - PDF: `[10.1016_j.ijar.2008.11.006].pdf`
    - 提取主题: `Generative Model; Supervised Learning; Unsupervised Learning; Boltzmann Machine; Probabilistic Model`
```

📊 处理统计

| 处理方式 | 条目数 | 成功率 | 说明 |
|---------|--------|--------|------|
| DOI匹配 | 20 | 100% | 从CSV文件Manual/Automatic Tags提取 |
| PDF分析 | 10 | 100% | 通过PyPDF2和pdfplumber分析原文 |
| **总计** | **30** | **100%** | **所有条目主题完整** |

热门主题排行（出现频次）

1. **Neural Network**: 7次
2. **Backpropagation**: 5次
3. **Discriminative model**: 4次
4. **Deep belief network**: 4次
5. **Generative model**: 4次
6. **Deep Learning**: 3次
7. **Speech Recognition**: 3次
8. **Deep Neural Networks**: 3次
9. **Initialization**: 2次
10. **Restricted Boltzmann machine**: 2次

## ④ 完善 (Enhancement):

1.获取某学术数据库快速检索的方法
```
提示词：我有一个文件：@csv文件，里面包含 30 篇论文的题录信息（含 DOI）。  
请问，有没有快速的方法能把这些 DOI 或其他元数据批量导入 Web of Science (WOS)，
以便一次性检索并获取这些论文的完整记录？ 
```

按照提示词生成了检索语句
```markdown
DO="10.1038/306021a0" OR DO="10.1109/icassp.2013.6638312" OR DO="10.1109/taslp.2014.2303296" OR DO="10.1162/neco/a/00311" OR DO="10.1145/3448250" OR DO="10.1038/355161a0" OR DO="10.1038/scientificamerican0992-144" OR DO="10.1016/0893-6080(90)90044-l" OR DO="10.1001/jama.2018.11100" OR DO="10.1162/neco.1992.4.4.473" OR DO="10.1006/nimg.2002.1090" OR DO="10.4249/scholarpedia.5947" OR DO="10.1145/168304.168306" OR DO="10.1126/science.7761831" OR DO="10.1109/icassp.2013.6639344" OR DO="10.1162/neco.1995.7.5.889" OR DO="10.1109/icassp.2013.6639346" OR DO="10.1016/j.ijar.2008.11.006" OR DO="10.1016/0004-3702(89)90049-0" OR DO="10.1109/tasl.2011.2109382" OR DO="10.1145/1273496.1273596" OR DO="10.1109/29.21701" OR DO="10.1207/s15516709cog0901/7" OR DO="10.1162/neco.1991.3.1.79" OR DO="10.1109/icassp.2013.6638947" OR DO="10.1109/msp.2012.2205597" OR DO="10.1162/neco.2006.18.7.1527" OR DO="10.1126/science.1127647" OR DO="10.1038/323533a0" OR DO="10.1145/3065386"
```

WOS显示17个结果，聊胜于无。导出Excel格式。

2.合并不同来源的元数据

```
提示词：请将 @xlsx文件 中的 “Author Keywords” 和 “Keywords Plus” 两列，  
合并到 @xlsx文件 中，  
新列分别命名为 author_keywords_wos 和 keywords_plus_wos。  
匹配时，请基于 DOI 精确匹配；若 DOI 缺失，则基于标题相似度（模糊匹配）对齐记录。  
完成后，生成一个包含所有原始列及新增两列的新的 Excel 文件。
```

📊 匹配结果统计

1.总体匹配情况：
| 匹配类型 | 记录数 | 比例 |
|---------|--------|------|
| DOI精确匹配 | 17条 | 56.7% |
| 标题模糊匹配 | 0条 | 0.0% |
| 无匹配 | 13条 | 43.3% |

2.数据覆盖情况：
| 新增列 | 有效数据记录数 | 覆盖率 |
|--------|----------------|--------|
| author_keywords_wos | 4条 | 13.3% |
| keywords_plus_wos | 6条 | 20.0% |

成功匹配以及合并记录。

3.给md文件加上元数据标签
```markdown
提示词：
请将一个CSV文件@sampled_tweets.csv 中的每一行内容，转换为一个独立的Markdown（.md）文档。  
每个文档需要在开头生成以下元标签（YAML Front Matter），并根据CSV数据和规则补全内容：

---
title:    {根据时间列转换为中文日期，例如 "二零二零年三月二十一日 二十三点二分"}
excerpt:  "{从CSV中的question字段提取前N字作为摘要，自动加双引号}"
layout:   defult   # 固定为 defult
category: {CSV中type列的值}
keywords: [{使用KeyBERT从正文中提取关键词列表}]
source:   [X@LFeldmanBarrett]   # 固定不变
uin:      {根据时间列生成唯一编码，例如20200321230229}
---

要求：
1. 从CSV中的相应字段提取数据（如type、时间、正文等）。
2. 时间需转换为中文大写数字日期，并补充具体时间（小时、分钟）。
3. excerpt 自动截取指定长度（如前50字），加上双引号。
4. keywords 使用KeyBERT对正文提取，结果为数组格式。
5. 每行CSV生成一个单独的.md文件，文件名建议使用uin值。
6. 输出结果保持Markdown格式。
```
处理统计
- 处理时间: 2025年07月27日 14:21:19
- 输入文件: data/raw/dataset_twitter-Hinton-tweets-scraper_2025-07-27.csv
- 输出目录: data/processed/tweets_md_v2
- 总推文数: 73
- 成功处理: 73
- 处理失败: 0

处理说明
- 时间格式已转换为中文格式
- 摘要自动截取前50字符
- 关键词通过KeyBERT自动提取
- 分类根据推文内容自动推断
- 每个推文生成独立的Markdown文件

生成的字段说明
- title: 中文时间格式
- excerpt: 推文摘要（前50字符）
- layout: 固定为 "defult"
- category: 根据内容推断的分类
- keywords: KeyBERT提取的关键词列表
- source: 固定为 [X@geoffreyhinton]
- uin: 基于时间的唯一编码（YYYYMMDDHHMMSS）

---

## 思考：
- 清洗过后的数据更容易分析，不然会给分析造成很大的成本负担以及干扰。但是清洗数据的过程中，往往需要花费大量的时间，而且清洗的结果是否准确，还需要反复验证。

## 复盘:
- 有个几个坑需要注意，第一个是环境依赖问题，解决方案是，我在Cursor的User Rules里备注了```当发现环境依赖缺失时，优先激活/Users/tommy/Projects/.venv虚拟环境，并将包安装在此环境中。安装时使用清华大学镜像源：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple```，基本没有报错了。
- 第二个是，命名文件夹的时候最好不要用中文，```pathlib.Path.as_uri()```方法会将中文字符进行URL编码，这导致Excel无法正确识别路径。从而影响xlsx文件建立超链接直接点开对应名称的文件。
- 第三个，运行智能整理的过程中，会产生大量py脚本文件，用完最好及时删除，不然容易混乱。

整理数据过程中产出了大量的py脚本
![](https://s21.ax1x.com/2025/07/27/pVYpVnx.png)

- 第四个，生成或修改后的数据一定要手动验证，往往出错的地方是很小很细节的，比如```/```、```\```、```_```，等符合问题需要注意。

## 同伴反馈
- 建议认真阅读上一位同学的作业，并提出1个你认为最有启发的亮点和1个具体的改进建议。


## Changelog:
- 20250727 彭淞完成初稿 【08:00~11:00； 13:00~15:00】





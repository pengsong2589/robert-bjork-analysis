#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Geoffrey Hintonæœºæ„ä¿¡æ¯æŸ¥è¯¢è„šæœ¬ï¼ˆæœ€ç»ˆç‰ˆï¼‰
åŠŸèƒ½ï¼šä»CSVæ–‡ä»¶ä¸­æå–Geoffrey Hintonçš„è®ºæ–‡ï¼ŒæŸ¥è¯¢OpenAlex APIè·å–æœºæ„ä¿¡æ¯ï¼Œç”ŸæˆExcelæ–‡ä»¶

æœ€ç»ˆç‰ˆç‰¹æ€§ï¼š
1. å¤„ç†æ‰€æœ‰è®ºæ–‡ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
2. ä¿®å¤DOIæ ¼å¼é—®é¢˜
3. å®Œæ•´çš„æœºæ„ä¿¡æ¯æå–
4. ä¼˜åŒ–çš„Excelè¾“å‡ºæ ¼å¼
5. è‡ªåŠ¨å¤‡ä»½åŠŸèƒ½

ä½œè€…ï¼šAIåŠ©æ‰‹
ç‰ˆæœ¬ï¼š3.0 - æœ€ç»ˆç‰ˆ
åˆ›å»ºæ—¶é—´ï¼š2025-01-27
"""

import pandas as pd
import requests
import time
import os
import shutil
from datetime import datetime
import logging
from pathlib import Path
import re

# é…ç½®æ—¥å¿—
log_filename = f"hinton_affiliations_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class HintonAffiliationsFinalQuery:
    """Geoffrey Hintonæœºæ„ä¿¡æ¯æŸ¥è¯¢ç±»ï¼ˆæœ€ç»ˆç‰ˆï¼‰"""
    
    def __init__(self, csv_file_path, pdf_dir_path):
        """
        åˆå§‹åŒ–æŸ¥è¯¢å™¨
        
        Args:
            csv_file_path (str): è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
            pdf_dir_path (str): PDFæ–‡ä»¶ç›®å½•è·¯å¾„
        """
        self.csv_file_path = csv_file_path
        self.pdf_dir_path = pdf_dir_path
        self.base_url = "https://api.openalex.org/works"
        
        # ç»“æœå­˜å‚¨
        self.results = []
        self.stats = {
            'total_papers': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'api_errors': 0
        }
        
    def backup_existing_files(self):
        """å¤‡ä»½ç°æœ‰çš„ç»“æœæ–‡ä»¶"""
        try:
            backup_dir = "data/backup"
            os.makedirs(backup_dir, exist_ok=True)
            
            # æŸ¥æ‰¾ç°æœ‰çš„ç»“æœæ–‡ä»¶
            existing_files = list(Path("data/processed").glob("Geoffrey_Hinton_æœºæ„ä¿¡æ¯_*.xlsx"))
            
            for file_path in existing_files:
                backup_path = os.path.join(backup_dir, f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file_path.name}")
                shutil.copy2(str(file_path), backup_path)
                logger.info(f"å·²å¤‡ä»½æ–‡ä»¶: {file_path.name} -> {backup_path}")
                
        except Exception as e:
            logger.warning(f"å¤‡ä»½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def normalize_doi(self, doi):
        """
        æ ‡å‡†åŒ–DOIæ ¼å¼
        å°†ä¸‹åˆ’çº¿æ ¼å¼çš„DOIè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        
        Args:
            doi (str): åŸå§‹DOIå­—ç¬¦ä¸²
            
        Returns:
            str: æ ‡å‡†åŒ–åçš„DOI
        """
        if not doi:
            return ""
        
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œç©ºæ ¼
        doi = doi.strip()
        if doi.startswith('doi:'):
            doi = doi[4:]
        if doi.startswith('DOI:'):
            doi = doi[4:]
        
        # å°†ç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿è½¬æ¢ä¸ºæ–œæ 
        if '_' in doi and '/' not in doi:
            first_underscore = doi.find('_')
            if first_underscore > 0:
                doi = doi[:first_underscore] + '/' + doi[first_underscore+1:]
        
        return doi
    
    def read_csv_data(self):
        """è¯»å–CSVæ–‡ä»¶æ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨è¯»å–CSVæ–‡ä»¶: {self.csv_file_path}")
            df = pd.read_csv(self.csv_file_path, encoding='utf-8')
            logger.info(f"æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•")
            return df
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def filter_hinton_papers(self, df):
        """ç­›é€‰Geoffrey Hintonä½œä¸ºä½œè€…çš„è®ºæ–‡"""
        try:
            hinton_papers = df[df['Author'].str.contains('Hinton, Geoffrey', na=False)]
            self.stats['total_papers'] = len(hinton_papers)
            logger.info(f"æ‰¾åˆ° {len(hinton_papers)} ç¯‡Geoffrey Hintonçš„è®ºæ–‡")
            
            return hinton_papers
        except Exception as e:
            logger.error(f"ç­›é€‰è®ºæ–‡å¤±è´¥: {e}")
            raise
    
    def query_openalex_by_doi(self, original_doi):
        """
        é€šè¿‡DOIæŸ¥è¯¢OpenAlex APIè·å–æœºæ„ä¿¡æ¯
        
        Args:
            original_doi (str): åŸå§‹DOI
            
        Returns:
            dict: åŒ…å«æœºæ„ä¿¡æ¯å’Œå…¶ä»–å…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            # æ ‡å‡†åŒ–DOI
            doi = self.normalize_doi(original_doi)
            
            # å°è¯•å¤šç§æŸ¥è¯¢æ–¹å¼
            query_urls = [
                f"{self.base_url}?filter=doi:{doi}",
                f"{self.base_url}?search={doi.replace('/', '%2F')}",
                f"{self.base_url}?filter=doi:https://doi.org/{doi}"
            ]
            
            logger.info(f"æ­£åœ¨æŸ¥è¯¢DOI: {original_doi} -> {doi}")
            
            # å‘é€è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'HintonAffiliationsQuery/3.0 (academic-research@example.com)',
                'Accept': 'application/json'
            }
            
            for i, url in enumerate(query_urls):
                try:
                    response = requests.get(url, headers=headers, timeout=20)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if data.get('results'):
                            work = data['results'][0]
                            
                            # æå–åŸºæœ¬ä¿¡æ¯
                            paper_info = {
                                'title': work.get('title', 'Unknown'),
                                'publication_year': work.get('publication_year'),
                                'publication_date': work.get('publication_date'),
                                'venue': '',
                                'citations_count': work.get('cited_by_count', 0)
                            }
                            
                            # æå–æœŸåˆŠ/ä¼šè®®ä¿¡æ¯
                            if work.get('primary_location', {}).get('source'):
                                source = work['primary_location']['source']
                                paper_info['venue'] = source.get('display_name', '')
                            
                            # æŸ¥æ‰¾Geoffrey Hintonçš„æœºæ„ä¿¡æ¯
                            hinton_affiliations = []
                            
                            for authorship in work.get('authorships', []):
                                author = authorship.get('author', {})
                                author_name = author.get('display_name', '')
                                
                                # æ£€æŸ¥æ˜¯å¦ä¸ºGeoffrey Hinton
                                if self.is_hinton_author(author_name):
                                    logger.info(f"æ‰¾åˆ°åŒ¹é…ä½œè€…: {author_name}")
                                    
                                    # æå–æœºæ„ä¿¡æ¯
                                    institutions = authorship.get('institutions', [])
                                    for inst in institutions:
                                        inst_name = inst.get('display_name', 'Unknown Institution')
                                        inst_country = inst.get('country_code', '')
                                        inst_type = inst.get('type', '')
                                        
                                        # æ„å»ºå®Œæ•´çš„æœºæ„ä¿¡æ¯
                                        full_inst_name = inst_name
                                        if inst_country:
                                            full_inst_name += f" ({inst_country})"
                                        
                                        hinton_affiliations.append({
                                            'name': inst_name,
                                            'full_name': full_inst_name,
                                            'country': inst_country,
                                            'type': inst_type
                                        })
                            
                            if hinton_affiliations:
                                logger.info(f"æ‰¾åˆ° {len(hinton_affiliations)} ä¸ªæœºæ„ä¿¡æ¯")
                                self.stats['successful_queries'] += 1
                                
                                return {
                                    'affiliations': hinton_affiliations,
                                    'paper_info': paper_info,
                                    'status': 'success'
                                }
                            else:
                                logger.warning(f"åœ¨è®ºæ–‡ä¸­æœªæ‰¾åˆ°Geoffrey Hintonçš„æœºæ„ä¿¡æ¯")
                                
                except requests.exceptions.RequestException as e:
                    logger.debug(f"æŸ¥è¯¢æ–¹æ³• {i+1} è¯·æ±‚å¼‚å¸¸: {e}")
                    continue
                
                # é¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(0.5)
            
            self.stats['failed_queries'] += 1
            logger.warning(f"æ‰€æœ‰æŸ¥è¯¢æ–¹æ³•éƒ½æœªæ‰¾åˆ°DOI {original_doi} çš„ç›¸å…³ä¿¡æ¯")
            return {'status': 'not_found'}
                
        except Exception as e:
            self.stats['api_errors'] += 1
            logger.error(f"æŸ¥è¯¢DOI {original_doi} æ—¶å‡ºé”™: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def is_hinton_author(self, author_name):
        """åˆ¤æ–­ä½œè€…æ˜¯å¦ä¸ºGeoffrey Hinton"""
        if not author_name:
            return False
        
        author_name_lower = author_name.lower()
        
        # å¤šç§å¯èƒ½çš„å§“åå˜ä½“
        hinton_variants = [
            'geoffrey hinton',
            'geoffrey e. hinton',
            'geoffrey everest hinton',
            'g. hinton',
            'g.e. hinton',
            'hinton, geoffrey',
            'hinton, g',
            'hinton, g.',
            'hinton, g.e.'
        ]
        
        for variant in hinton_variants:
            if variant in author_name_lower:
                return True
        
        # æ›´å®½æ³›çš„åŒ¹é…ï¼šåŒ…å«geoffreyå’Œhinton
        return 'geoffrey' in author_name_lower and 'hinton' in author_name_lower
    
    def get_pdf_link(self, doi):
        """æ ¹æ®DOIç”ŸæˆPDFæ–‡ä»¶é“¾æ¥"""
        try:
            # ä½¿ç”¨åŸå§‹DOIæ ¼å¼ï¼ˆå¸¦ä¸‹åˆ’çº¿ï¼‰ä½œä¸ºæ–‡ä»¶å
            pdf_filename = f"[{doi}].pdf"
            pdf_path = os.path.join(self.pdf_dir_path, pdf_filename)
            
            if os.path.exists(pdf_path):
                absolute_path = os.path.abspath(pdf_path)
                return f"file://{absolute_path}"
            else:
                return "PDFæ–‡ä»¶æœªæ‰¾åˆ°"
                
        except Exception as e:
            logger.error(f"ç”ŸæˆPDFé“¾æ¥æ—¶å‡ºé”™: {e}")
            return "é“¾æ¥ç”Ÿæˆå¤±è´¥"
    
    def process_papers(self, hinton_papers):
        """å¤„ç†Geoffrey Hintonçš„è®ºæ–‡"""
        logger.info("å¼€å§‹å¤„ç†æ‰€æœ‰è®ºæ–‡æ•°æ®...")
        
        for index, paper in hinton_papers.iterrows():
            try:
                # æå–åŸºæœ¬ä¿¡æ¯
                author = paper.get('Author', '')
                title = paper.get('Title', '')
                doi = paper.get('DOI', '')
                pub_year = paper.get('Publication Year', '')
                pub_date = paper.get('Date', '')
                publication_title = paper.get('Publication Title', '')
                
                logger.info(f"å¤„ç†ç¬¬ {len(self.results)+1}/{self.stats['total_papers']} ç¯‡è®ºæ–‡: {title[:50]}...")
                
                # æŸ¥è¯¢æœºæ„ä¿¡æ¯
                query_result = self.query_openalex_by_doi(doi)
                
                # å¤„ç†æŸ¥è¯¢ç»“æœ
                if query_result['status'] == 'success':
                    affiliations = query_result['affiliations']
                    paper_info = query_result['paper_info']
                    
                    # æ„å»ºæœºæ„å­—ç¬¦ä¸²
                    affiliations_str = " | ".join([aff['full_name'] for aff in affiliations])
                    
                    # ä½¿ç”¨APIè¿”å›çš„æ›´å‡†ç¡®ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    actual_pub_year = paper_info.get('publication_year') or pub_year
                    actual_venue = paper_info.get('venue') or publication_title
                    citations = paper_info.get('citations_count', 0)
                    
                else:
                    affiliations_str = "æœºæ„ä¿¡æ¯æœªæ‰¾åˆ°"
                    actual_pub_year = pub_year
                    actual_venue = publication_title
                    citations = 0
                
                # ç”ŸæˆPDFé“¾æ¥
                pdf_link = self.get_pdf_link(doi)
                
                # å­˜å‚¨ç»“æœ
                result = {
                    'ä½œè€…': author,
                    'æ ‡é¢˜': title,
                    'DOI': doi,
                    'æ ‡å‡†åŒ–DOI': self.normalize_doi(doi),
                    'å‘è¡¨å¹´ä»½': actual_pub_year,
                    'å‘è¡¨æ—¥æœŸ': pub_date,
                    'æœŸåˆŠ/ä¼šè®®': actual_venue,
                    'å¼•ç”¨æ¬¡æ•°': citations,
                    'æœºæ„': affiliations_str,
                    'PDFé“¾æ¥': pdf_link,
                    'æŸ¥è¯¢çŠ¶æ€': query_result['status']
                }
                
                self.results.append(result)
                
                # æ¯5ç¯‡è®ºæ–‡æ‰“å°ä¸€æ¬¡è¿›åº¦
                if len(self.results) % 5 == 0:
                    logger.info(f"å·²å¤„ç† {len(self.results)}/{self.stats['total_papers']} ç¯‡è®ºæ–‡")
                
                # ä¸ºé¿å…é¢‘ç¹è¯·æ±‚ï¼Œæ·»åŠ å»¶è¿Ÿ
                time.sleep(1.5)
                
            except Exception as e:
                logger.error(f"å¤„ç†è®ºæ–‡ {index} æ—¶å‡ºé”™: {e}")
                continue
    
    def save_to_excel(self, output_file):
        """ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶: {output_file}")
            
            if not self.results:
                logger.warning("æ²¡æœ‰ç»“æœå¯ä»¥ä¿å­˜")
                return
            
            # åˆ›å»ºDataFrame
            df_results = pd.DataFrame(self.results)
            
            # ä½¿ç”¨openpyxlå¼•æ“ä¿å­˜
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # ä¸»æ•°æ®è¡¨
                df_results.to_excel(writer, sheet_name='Hintonè®ºæ–‡æœºæ„ä¿¡æ¯', index=False)
                
                # ç»Ÿè®¡ä¿¡æ¯è¡¨
                stats_data = {
                    'ç»Ÿè®¡é¡¹ç›®': ['è®ºæ–‡æ€»æ•°', 'æˆåŠŸæŸ¥è¯¢', 'æŸ¥è¯¢å¤±è´¥', 'APIé”™è¯¯', 'æˆåŠŸç‡'],
                    'æ•°å€¼': [
                        self.stats['total_papers'],
                        self.stats['successful_queries'],
                        self.stats['failed_queries'],
                        self.stats['api_errors'],
                        f"{(self.stats['successful_queries']/self.stats['total_papers']*100):.1f}%" if self.stats['total_papers'] > 0 else "0%"
                    ]
                }
                df_stats = pd.DataFrame(stats_data)
                df_stats.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
                
                # è°ƒæ•´ä¸»è¡¨åˆ—å®½
                worksheet = writer.sheets['Hintonè®ºæ–‡æœºæ„ä¿¡æ¯']
                for column in worksheet.columns:
                    max_length = 0
                    column_name = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 80)
                    worksheet.column_dimensions[column_name].width = adjusted_width
                
                # æ·»åŠ è¶…é“¾æ¥åŠŸèƒ½åˆ°PDFé“¾æ¥åˆ—
                pdf_col = df_results.columns.get_loc('PDFé“¾æ¥') + 1
                for row in range(2, len(self.results) + 2):
                    cell = worksheet.cell(row=row, column=pdf_col)
                    if cell.value and cell.value.startswith('file://'):
                        cell.hyperlink = cell.value
                        cell.style = "Hyperlink"
            
            logger.info(f"æˆåŠŸä¿å­˜ {len(self.results)} æ¡è®°å½•åˆ°Excelæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        try:
            # ç»Ÿè®¡ä¸åŒæœºæ„çš„å‡ºç°æ¬¡æ•°
            affiliations_count = {}
            
            for result in self.results:
                if result['æŸ¥è¯¢çŠ¶æ€'] == 'success':
                    affiliations = result['æœºæ„'].split(' | ')
                    for aff in affiliations:
                        # æå–æœºæ„åç§°ï¼ˆå»é™¤å›½å®¶ä»£ç ï¼‰
                        clean_aff = aff.split(' (')[0]
                        affiliations_count[clean_aff] = affiliations_count.get(clean_aff, 0) + 1
            
            # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
            sorted_affiliations = sorted(affiliations_count.items(), key=lambda x: x[1], reverse=True)
            
            logger.info("\n=== æ€»ç»“æŠ¥å‘Š ===")
            logger.info(f"è®ºæ–‡æ€»æ•°: {self.stats['total_papers']}")
            logger.info(f"æˆåŠŸæŸ¥è¯¢: {self.stats['successful_queries']}")
            logger.info(f"æŸ¥è¯¢å¤±è´¥: {self.stats['failed_queries']}")
            logger.info(f"æˆåŠŸç‡: {(self.stats['successful_queries']/self.stats['total_papers']*100):.1f}%")
            
            logger.info("\nä¸»è¦å…³è”æœºæ„:")
            for aff, count in sorted_affiliations[:10]:
                logger.info(f"  {aff}: {count} ç¯‡è®ºæ–‡")
            
            return sorted_affiliations
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»ç»“æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            return []
    
    def run(self, output_file):
        """æ‰§è¡Œå®Œæ•´çš„æŸ¥è¯¢æµç¨‹"""
        try:
            logger.info("=== Geoffrey Hintonæœºæ„ä¿¡æ¯æŸ¥è¯¢å¼€å§‹ï¼ˆæœ€ç»ˆç‰ˆï¼‰ ===")
            
            # 0. å¤‡ä»½ç°æœ‰æ–‡ä»¶
            self.backup_existing_files()
            
            # 1. è¯»å–CSVæ•°æ®
            df = self.read_csv_data()
            
            # 2. ç­›é€‰Geoffrey Hintonçš„è®ºæ–‡
            hinton_papers = self.filter_hinton_papers(df)
            
            # 3. å¤„ç†è®ºæ–‡æ•°æ®
            self.process_papers(hinton_papers)
            
            # 4. ä¿å­˜åˆ°Excel
            self.save_to_excel(output_file)
            
            # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            affiliations_summary = self.generate_summary_report()
            
            logger.info("=== æŸ¥è¯¢å®Œæˆ ===")
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            return affiliations_summary
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ–‡ä»¶è·¯å¾„é…ç½®
        csv_file = "data/raw/202507271930_Hinton_Papers_30_v2.csv"
        pdf_dir = "data/processed/doi_renamed_pdfs_improved"
        output_file = f"data/processed/Geoffrey_Hinton_æœºæ„ä¿¡æ¯_å®Œæ•´ç‰ˆ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if not os.path.exists(csv_file):
            logger.error(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
            return
        
        if not os.path.exists(pdf_dir):
            logger.error(f"PDFç›®å½•ä¸å­˜åœ¨: {pdf_dir}")
            return
        
        # åˆ›å»ºæŸ¥è¯¢å™¨å¹¶æ‰§è¡Œ
        query = HintonAffiliationsFinalQuery(csv_file, pdf_dir)
        affiliations_summary = query.run(output_file)
        
        print(f"\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {output_file}")
        print(f"ğŸ“‹ å¤„ç†è®ºæ–‡æ•°é‡: {len(query.results)}")
        print(f"ğŸ¯ æˆåŠŸç‡: {(query.stats['successful_queries']/query.stats['total_papers']*100):.1f}%")
        
        # æ˜¾ç¤ºä¸»è¦æœºæ„ä¿¡æ¯
        if affiliations_summary:
            print(f"\nğŸ« Geoffrey Hintonä¸»è¦å…³è”æœºæ„:")
            for i, (aff, count) in enumerate(affiliations_summary[:5]):
                print(f"  {i+1}. {aff}: {count} ç¯‡è®ºæ–‡")
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main() 
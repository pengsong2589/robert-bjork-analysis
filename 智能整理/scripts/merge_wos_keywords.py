#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WOSå…³é”®è¯æ•°æ®åˆå¹¶è„šæœ¬
åŠŸèƒ½ï¼šå°†WOSæ–‡ä»¶ä¸­çš„Author Keywordså’ŒKeywords Plusåˆ—åˆå¹¶åˆ°ç°æœ‰Excelæ–‡ä»¶ä¸­
åŒ¹é…è§„åˆ™ï¼šä¼˜å…ˆDOIç²¾ç¡®åŒ¹é…ï¼Œå…¶æ¬¡æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2025-01-27
"""

import pandas as pd
import os
import shutil
from datetime import datetime
from fuzzywuzzy import fuzz
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('keyword_merge_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def create_backup(file_path):
    """
    åˆ›å»ºæ–‡ä»¶å¤‡ä»½
    Args:
        file_path (str): è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„
    Returns:
        str: å¤‡ä»½æ–‡ä»¶è·¯å¾„
    """
    if not os.path.exists(file_path):
        logging.error(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    backup_dir = "/Users/tommy/Projects/projects/AIå­¦æœ¯åˆ†æ/æ™ºèƒ½æ•´ç†/data/backup"
    os.makedirs(backup_dir, exist_ok=True)
    
    # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = os.path.basename(file_path)
    backup_path = os.path.join(backup_dir, f"backup_{timestamp}_{file_name}")
    
    # æ‰§è¡Œå¤‡ä»½
    shutil.copy2(file_path, backup_path)
    logging.info(f"å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
    return backup_path

def read_wos_file(wos_file_path):
    """
    è¯»å–WOS Excelæ–‡ä»¶
    Args:
        wos_file_path (str): WOSæ–‡ä»¶è·¯å¾„
    Returns:
        pandas.DataFrame: WOSæ•°æ®
    """
    try:
        # å°è¯•ä¸åŒçš„å¼•æ“è¯»å–Excelæ–‡ä»¶
        try:
            df = pd.read_excel(wos_file_path, engine='openpyxl')
        except:
            df = pd.read_excel(wos_file_path, engine='xlrd')
        
        logging.info(f"æˆåŠŸè¯»å–WOSæ–‡ä»¶: {wos_file_path}")
        logging.info(f"WOSæ–‡ä»¶åŒ…å« {len(df)} è¡Œæ•°æ®")
        logging.info(f"WOSæ–‡ä»¶åˆ—å: {list(df.columns)}")
        
        return df
    except Exception as e:
        logging.error(f"è¯»å–WOSæ–‡ä»¶å¤±è´¥: {e}")
        return None

def read_target_file(target_file_path):
    """
    è¯»å–ç›®æ ‡Excelæ–‡ä»¶
    Args:
        target_file_path (str): ç›®æ ‡æ–‡ä»¶è·¯å¾„
    Returns:
        pandas.DataFrame: ç›®æ ‡æ•°æ®
    """
    try:
        df = pd.read_excel(target_file_path, engine='openpyxl')
        logging.info(f"æˆåŠŸè¯»å–ç›®æ ‡æ–‡ä»¶: {target_file_path}")
        logging.info(f"ç›®æ ‡æ–‡ä»¶åŒ…å« {len(df)} è¡Œæ•°æ®")
        logging.info(f"ç›®æ ‡æ–‡ä»¶åˆ—å: {list(df.columns)}")
        
        return df
    except Exception as e:
        logging.error(f"è¯»å–ç›®æ ‡æ–‡ä»¶å¤±è´¥: {e}")
        return None

def normalize_doi(doi):
    """
    æ ‡å‡†åŒ–DOIæ ¼å¼
    Args:
        doi (str): åŸå§‹DOI
    Returns:
        str: æ ‡å‡†åŒ–åçš„DOI
    """
    if pd.isna(doi) or doi == "":
        return None
    
    doi = str(doi).strip().lower()
    # ç§»é™¤å¸¸è§çš„DOIå‰ç¼€
    prefixes = ['doi:', 'http://dx.doi.org/', 'https://dx.doi.org/', 
                'http://doi.org/', 'https://doi.org/']
    for prefix in prefixes:
        if doi.startswith(prefix):
            doi = doi[len(prefix):]
    
    return doi

def calculate_title_similarity(title1, title2):
    """
    è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
    Args:
        title1 (str): æ ‡é¢˜1
        title2 (str): æ ‡é¢˜2
    Returns:
        int: ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if pd.isna(title1) or pd.isna(title2):
        return 0
    
    title1 = str(title1).strip().lower()
    title2 = str(title2).strip().lower()
    
    return fuzz.ratio(title1, title2)

def find_matching_record(target_row, wos_df, doi_col_wos, title_col_wos, doi_col_target, title_col_target):
    """
    ä¸ºç›®æ ‡è®°å½•æŸ¥æ‰¾åŒ¹é…çš„WOSè®°å½•
    Args:
        target_row: ç›®æ ‡æ•°æ®è¡Œ
        wos_df: WOSæ•°æ®æ¡†
        doi_col_wos: WOSä¸­çš„DOIåˆ—å
        title_col_wos: WOSä¸­çš„æ ‡é¢˜åˆ—å
        doi_col_target: ç›®æ ‡æ–‡ä»¶ä¸­çš„DOIåˆ—å
        title_col_target: ç›®æ ‡æ–‡ä»¶ä¸­çš„æ ‡é¢˜åˆ—å
    Returns:
        tuple: (åŒ¹é…çš„è¡Œç´¢å¼•, åŒ¹é…ç±»å‹)
    """
    target_doi = normalize_doi(target_row.get(doi_col_target))
    target_title = target_row.get(title_col_target)
    
    # é¦–å…ˆå°è¯•DOIç²¾ç¡®åŒ¹é…
    if target_doi:
        for idx, wos_row in wos_df.iterrows():
            wos_doi = normalize_doi(wos_row.get(doi_col_wos))
            if wos_doi and target_doi == wos_doi:
                return idx, "DOI_EXACT"
    
    # DOIåŒ¹é…å¤±è´¥ï¼Œå°è¯•æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…
    if target_title:
        best_match_idx = None
        best_similarity = 0
        similarity_threshold = 85  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        for idx, wos_row in wos_df.iterrows():
            wos_title = wos_row.get(title_col_wos)
            similarity = calculate_title_similarity(target_title, wos_title)
            
            if similarity > best_similarity and similarity >= similarity_threshold:
                best_similarity = similarity
                best_match_idx = idx
        
        if best_match_idx is not None:
            return best_match_idx, f"TITLE_FUZZY_{best_similarity}"
    
    return None, "NO_MATCH"

def merge_keywords_data(wos_file_path, target_file_path, output_file_path):
    """
    åˆå¹¶å…³é”®è¯æ•°æ®çš„ä¸»å‡½æ•°
    Args:
        wos_file_path (str): WOSæ–‡ä»¶è·¯å¾„
        target_file_path (str): ç›®æ ‡æ–‡ä»¶è·¯å¾„
        output_file_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    # åˆ›å»ºå¤‡ä»½
    logging.info("å¼€å§‹åˆ›å»ºå¤‡ä»½...")
    backup_path = create_backup(target_file_path)
    if not backup_path:
        return False
    
    # è¯»å–æ–‡ä»¶
    logging.info("è¯»å–WOSæ–‡ä»¶...")
    wos_df = read_wos_file(wos_file_path)
    if wos_df is None:
        return False
    
    logging.info("è¯»å–ç›®æ ‡æ–‡ä»¶...")
    target_df = read_target_file(target_file_path)
    if target_df is None:
        return False
    
    # æ˜¾ç¤ºå¯ç”¨åˆ—åï¼Œå¸®åŠ©ç”¨æˆ·ç¡®è®¤
    print("\n=== WOSæ–‡ä»¶åˆ—å ===")
    for i, col in enumerate(wos_df.columns):
        print(f"{i+1:2d}. {col}")
    
    print("\n=== ç›®æ ‡æ–‡ä»¶åˆ—å ===")
    for i, col in enumerate(target_df.columns):
        print(f"{i+1:2d}. {col}")
    
    # è‡ªåŠ¨æ£€æµ‹å…³é”®åˆ—å
    author_keywords_col = None
    keywords_plus_col = None
    doi_col_wos = None
    title_col_wos = None
    doi_col_target = None
    title_col_target = None
    
    # åœ¨WOSæ–‡ä»¶ä¸­æŸ¥æ‰¾å…³é”®è¯åˆ—
    for col in wos_df.columns:
        col_lower = col.lower()
        if 'author keywords' == col_lower:
            author_keywords_col = col
        elif 'keywords plus' == col_lower:
            keywords_plus_col = col
        elif col_lower == 'doi':
            doi_col_wos = col
        elif 'article title' == col_lower:
            title_col_wos = col
    
    # åœ¨ç›®æ ‡æ–‡ä»¶ä¸­æŸ¥æ‰¾DOIå’Œæ ‡é¢˜åˆ—
    for col in target_df.columns:
        if col in ['DOI', 'æ ‡å‡†åŒ–DOI']:
            doi_col_target = col
        elif col in ['æ ‡é¢˜', 'Title']:
            title_col_target = col
    
    logging.info(f"æ£€æµ‹åˆ°çš„åˆ—åæ˜ å°„:")
    logging.info(f"  WOS Author Keywords: {author_keywords_col}")
    logging.info(f"  WOS Keywords Plus: {keywords_plus_col}")
    logging.info(f"  WOS DOI: {doi_col_wos}")
    logging.info(f"  WOS Title: {title_col_wos}")
    logging.info(f"  ç›®æ ‡DOI: {doi_col_target}")
    logging.info(f"  ç›®æ ‡Title: {title_col_target}")
    
    if not all([author_keywords_col, keywords_plus_col, doi_col_target, title_col_target]):
        logging.error("æœªèƒ½æ£€æµ‹åˆ°æ‰€æœ‰å¿…éœ€çš„åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
        return False
    
    # åˆå§‹åŒ–æ–°åˆ—
    target_df['author_keywords_wos'] = ""
    target_df['keywords_plus_wos'] = ""
    
    # è®°å½•åŒ¹é…ç»Ÿè®¡
    match_stats = {
        'DOI_EXACT': 0,
        'TITLE_FUZZY': 0,
        'NO_MATCH': 0
    }
    
    detailed_matches = []
    
    # æ‰§è¡ŒåŒ¹é…å’Œåˆå¹¶
    logging.info("å¼€å§‹åŒ¹é…å’Œåˆå¹¶æ•°æ®...")
    for idx, target_row in target_df.iterrows():
        match_idx, match_type = find_matching_record(
            target_row, wos_df, doi_col_wos, title_col_wos, 
            doi_col_target, title_col_target
        )
        
        if match_idx is not None:
            # å¤åˆ¶å…³é”®è¯æ•°æ®
            author_keywords = wos_df.loc[match_idx, author_keywords_col]
            keywords_plus = wos_df.loc[match_idx, keywords_plus_col]
            
            target_df.loc[idx, 'author_keywords_wos'] = author_keywords if pd.notna(author_keywords) else ""
            target_df.loc[idx, 'keywords_plus_wos'] = keywords_plus if pd.notna(keywords_plus) else ""
            
            # ç»Ÿè®¡åŒ¹é…ç±»å‹
            match_category = match_type.split('_')[0] + '_' + match_type.split('_')[1] if '_' in match_type else match_type
            if match_category.startswith('TITLE_FUZZY'):
                match_stats['TITLE_FUZZY'] += 1
            else:
                match_stats[match_category] += 1
            
            # è®°å½•è¯¦ç»†åŒ¹é…ä¿¡æ¯
            detailed_matches.append({
                'target_index': idx,
                'target_title': target_row.get(title_col_target, ''),
                'target_doi': target_row.get(doi_col_target, ''),
                'wos_index': match_idx,
                'wos_title': wos_df.loc[match_idx, title_col_wos] if title_col_wos else '',
                'wos_doi': wos_df.loc[match_idx, doi_col_wos] if doi_col_wos else '',
                'match_type': match_type,
                'author_keywords': author_keywords if pd.notna(author_keywords) else '',
                'keywords_plus': keywords_plus if pd.notna(keywords_plus) else ''
            })
        else:
            match_stats['NO_MATCH'] += 1
    
    # ä¿å­˜ç»“æœ
    logging.info(f"ä¿å­˜åˆå¹¶ç»“æœåˆ°: {output_file_path}")
    target_df.to_excel(output_file_path, index=False, engine='openpyxl')
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logging.info("\n=== åŒ¹é…ç»Ÿè®¡ ===")
    total_records = len(target_df)
    for match_type, count in match_stats.items():
        percentage = (count / total_records) * 100
        logging.info(f"{match_type}: {count} æ¡è®°å½• ({percentage:.1f}%)")
    
    # ä¿å­˜è¯¦ç»†åŒ¹é…æŠ¥å‘Š
    if detailed_matches:
        match_report_path = output_file_path.replace('.xlsx', '_match_report.xlsx')
        match_df = pd.DataFrame(detailed_matches)
        match_df.to_excel(match_report_path, index=False, engine='openpyxl')
        logging.info(f"è¯¦ç»†åŒ¹é…æŠ¥å‘Šå·²ä¿å­˜: {match_report_path}")
    
    logging.info("å…³é”®è¯åˆå¹¶ä»»åŠ¡å®Œæˆ!")
    return True

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„é…ç½®
    wos_file_path = "/Users/tommy/Projects/projects/AIå­¦æœ¯åˆ†æ/æ™ºèƒ½æ•´ç†/data/raw/202507271400-Geoffery-Hinton-WOS-17.xls"
    target_file_path = "/Users/tommy/Projects/projects/AIå­¦æœ¯åˆ†æ/æ™ºèƒ½æ•´ç†/data/processed/Hinton_with_Affiliations_v4.xlsx"
    output_file_path = "/Users/tommy/Projects/projects/AIå­¦æœ¯åˆ†æ/æ™ºèƒ½æ•´ç†/data/processed/Hinton_with_Affiliations_v5.xlsx"
    
    # æ‰§è¡Œåˆå¹¶æ“ä½œ
    success = merge_keywords_data(wos_file_path, target_file_path, output_file_path)
    
    if success:
        print(f"\nâœ… ä»»åŠ¡å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")
        print(f"ğŸ“ æ“ä½œæ—¥å¿—: keyword_merge_log.txt")
        print(f"ğŸ”„ å¤‡ä»½æ–‡ä»¶å·²åˆ›å»ºåœ¨ data/backup/ ç›®å½•ä¸­")
    else:
        print("\nâŒ ä»»åŠ¡å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯")

if __name__ == "__main__":
    main() 
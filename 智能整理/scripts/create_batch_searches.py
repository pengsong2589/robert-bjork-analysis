#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DOIåˆ†æ‰¹æ£€ç´¢è„šæœ¬
å°†30ä¸ªDOIåˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡ï¼Œä¾¿äºåœ¨Web of Scienceä¸­åˆ†åˆ«æ£€ç´¢

åˆ›å»ºæ—¶é—´ï¼š2025-01-27
ä½œè€…ï¼šAIåŠ©æ‰‹
"""

import pandas as pd
import os
from datetime import datetime

def create_batch_searches():
    """åˆ›å»ºåˆ†æ‰¹æ£€ç´¢è¯­å¥"""
    
    # è¯»å–åŸå§‹CSVæ–‡ä»¶
    csv_file = "data/raw/202507271930_Hinton_Papers_30_v2.csv"
    df = pd.read_csv(csv_file)
    
    # æå–æ‰€æœ‰DOI
    dois = []
    for index, row in df.iterrows():
        doi = row.get('DOI', '').strip()
        if doi:
            dois.append(doi)
    
    # åˆ†æ‰¹è®¾ç½®ï¼šæ¯æ‰¹5ä¸ªDOI
    batch_size = 5
    total_dois = len(dois)
    num_batches = (total_dois + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
    
    output_dir = "data/processed/wos_import/batch_searches"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"æ€»å…± {total_dois} ä¸ªDOIï¼Œå°†åˆ†æˆ {num_batches} æ‰¹")
    
    # ç”Ÿæˆæ¯æ‰¹æ£€ç´¢è¯­å¥
    batch_info = []
    
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, total_dois)
        batch_dois = dois[start_idx:end_idx]
        
        # åˆ›å»ºæ£€ç´¢è¯­å¥
        search_query = ' OR '.join([f'DO="{doi}"' for doi in batch_dois])
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        batch_filename = f"batch_{i+1:02d}_search_{timestamp}.txt"
        with open(os.path.join(output_dir, batch_filename), 'w', encoding='utf-8') as f:
            f.write(f"=== ç¬¬ {i+1} æ‰¹ DOI æ£€ç´¢è¯­å¥ ===\n")
            f.write(f"åŒ…å« {len(batch_dois)} ä¸ªDOI (åºå· {start_idx+1}-{end_idx})\n\n")
            f.write("å¤åˆ¶ä¸‹é¢çš„è¯­å¥åˆ°Web of Scienceé«˜çº§æ£€ç´¢ï¼š\n\n")
            f.write(search_query)
            f.write(f"\n\nåŒ…å«çš„DOI:\n")
            for j, doi in enumerate(batch_dois, 1):
                f.write(f"{j}. {doi}\n")
        
        batch_info.append({
            'batch': i+1,
            'filename': batch_filename,
            'doi_count': len(batch_dois),
            'start_idx': start_idx+1,
            'end_idx': end_idx
        })
        
        print(f"æ‰¹æ¬¡ {i+1}: {len(batch_dois)} ä¸ªDOI -> {batch_filename}")
    
    # åˆ›å»ºæ€»è§ˆæ–‡ä»¶
    summary_file = os.path.join(output_dir, f"batch_summary_{timestamp}.md")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("# DOIåˆ†æ‰¹æ£€ç´¢æ€»è§ˆ\n\n")
        f.write(f"**æ€»DOIæ•°é‡**: {total_dois}\n")
        f.write(f"**æ‰¹æ¬¡æ•°é‡**: {num_batches}\n")
        f.write(f"**æ¯æ‰¹å¤§å°**: æœ€å¤š{batch_size}ä¸ªDOI\n\n")
        
        f.write("## ä½¿ç”¨æ­¥éª¤\n\n")
        f.write("1. æŒ‰é¡ºåºä½¿ç”¨æ¯ä¸ªæ‰¹æ¬¡çš„æ£€ç´¢è¯­å¥\n")
        f.write("2. åœ¨Web of Scienceä¸­é€ä¸€æ£€ç´¢\n")
        f.write("3. å°†æ‰€æœ‰ç»“æœå¯¼å‡ºå¹¶åˆå¹¶\n\n")
        
        f.write("## æ‰¹æ¬¡è¯¦æƒ…\n\n")
        f.write("| æ‰¹æ¬¡ | æ–‡ä»¶å | DOIæ•°é‡ | åºå·èŒƒå›´ |\n")
        f.write("|------|--------|---------|----------|\n")
        
        for info in batch_info:
            f.write(f"| {info['batch']} | {info['filename']} | {info['doi_count']} | {info['start_idx']}-{info['end_idx']} |\n")
        
        f.write("\n## æ³¨æ„äº‹é¡¹\n\n")
        f.write("- å»ºè®®æŒ‰æ‰¹æ¬¡é¡ºåºæ£€ç´¢ï¼Œé¿å…é—æ¼\n")
        f.write("- æ¯æ¬¡æ£€ç´¢åå»ºè®®å¯¼å‡ºç»“æœï¼Œæœ€ååˆå¹¶\n")
        f.write("- å¦‚æœæŸæ‰¹æ¬¡æ£€ç´¢ç»“æœä¸ºç©ºï¼Œæ£€æŸ¥DOIæ ¼å¼æ˜¯å¦æ­£ç¡®\n")
    
    print(f"\nâœ… åˆ†æ‰¹æ£€ç´¢æ–‡ä»¶å·²ç”Ÿæˆåœ¨: {output_dir}")
    print(f"ğŸ“‹ æ€»è§ˆæ–‡ä»¶: batch_summary_{timestamp}.md")
    
    return output_dir

if __name__ == "__main__":
    create_batch_searches() 
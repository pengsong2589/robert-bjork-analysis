## 复刻老师实操演练，所遭遇的“坑”以及带来的思考

### 实操前的准备工作
#### Zotero管理信息的优缺点

|     | 优点           | 缺点           |
| --- | -------------- | -------------- |
| 01  | 元数据类型丰富 | 元数据规范有限 |
| 02  | 开源免费       | 运行速度慢     |
| 03  | 第三方插件多   | 过度依赖插件   |

#### 回归不变的事物-本地文件
Zotero及其生态 → 本地文件 ← IDE（Trae、Cursor）

#### Cursor基础操作
- 基本设置：① 登录
    - 成功
- 基本设置：② 选择模型
    - 我开通了会员，成功
- 基本设置：③ 允许Cursor自动运行
    - 成功
- 基本设置：④ 设置User Rules
    - 未设置，星宇老师说可以智能分析时再设置
- 基本设置：⑤ 设置网络
    - 成功
- 基本设置：⑥ 创建项目文件夹
    - 提示报错: ```mkdir -p data/{raw,interim,processed} scripts && touch README.md ```
    - 原因：根本原因在于它使用了 Bash 或 Zsh 等 Unix/Linux shell 的特有语法和命令，而这些是 Windows 终端无法直接解析的。
    - 解决方案如下：

```markdown
# PowerShell 的 mkdir 可以直接处理一个字符串数组
mkdir "data\raw", "data\interim", "data\processed", "scripts"
# 创建空文件
New-Item -ItemType File -Name "README.md"

# 可以将它们写在一行，用分号 ; 隔开
mkdir "data\raw", "data\interim", "data\processed", "scripts"; New-Item "README.md"
```

### 一、编码
#### 1.获取缺失的论文DOI

> 步骤：导出Zotero元数据 → 使用AI编辑器对话，联网自动获取DOI → 让大模型补全DOI

具体实操过程：
01 尝试导出Zotero中**Geoffrey Hinton**的30条Papers元数据，格式.csv，命名"Ge202507241429-Geoffrey Hinton-30-papers.csv"；
> 查看数据，有15条DOI缺失



02 使用Gemini-2.5-pro分析数据并补充DOI数据
> 参考提示词：这是一份论文元数据表格，部分记录缺少 DOI。请根据论文的标题、作者、期刊等元数据联网检索并补全缺失的DOI。


根据搜索结果，AI找到了以下DOI信息：
> 1.成功找到DOI的论文：
"Representation and control in vision" (1978)
找到DOI: 10.1007/978-3-642-82017-5_10
来源: Springer章节，收录在《Pictorial Data Analysis》NATO ASI Series中

>2.找到相关信息但无DOI的论文：
"How Learning Can Guide Evolution" (1987)
状态: Complex Systems期刊原文，被Nature评论文章引用
相关DOI: 10.1038/329761a0 (Maynard Smith的Nature评论文章)
说明: 原始论文可能没有DOI，因为是早期期刊文章
"Experiments on Learning by Back Propagation" (1986)
状态: 技术报告 CMU-CS-86-126
说明: 作为技术报告发布，通常没有DOI
"Massively parallel architectures for AI: netl, thistle, and boltzmann machines" (1983)
状态: AAAI-83会议论文
相关信息: 在Springer章节中被引用 (10.1007/978-1-4613-1989-4_1)
说明: 早期会议论文通常没有DOI

> 3.确认无DOI的论文类型：
早期会议论文 (1980年代之前的AAAI、IJCAI等)
"Shape recognition and illusory conjunctions" (IJCAI-85)
"Learning in massively parallel nets" (AAAI-86)
"Learning in Massively Parallel Nets (Panel)" (AAAI-86)
早期NIPS论文
"Learning Representations by Recirculation" (NIPS-87)
说明: 早期NIPS论文通常没有DOI系统
技术报告和会议论文集
"OPTIMAL PERCEPTUAL INFERENCE" (1983)
"Using relaxation to find a puppet" (AISB-76)
"LEARNING SEMANTIC FEATURES" (1984)
特殊情况
"APPRENTISSAGE DANS LES MACHINES DE BOLTZMANN" (1986) - 法语发表
"Preventive psychotherapeutic measures..." (1982) - 可能有DOI但需要进一步验证

03 总结：
- 总共缺失DOI的记录: 15条
- 成功找到DOI: 1条
- 找到相关信息但确认无DOI: 14条
- 主要原因: 早期会议论文、技术报告、期刊文章在DOI系统建立之前发表

04 AI创建py脚本（使用```pandas```库读取CSV文件→修改信息→新文件存入/data/processed/目录下），来读取原始CSV文件，然后修改，将新结果写入新文件```Ge202507241429-Geoffrey_Hinton-30-papers_with_DOI.csv```，然后**提示报错**，原因是**环境**中没有```pandas```库。
> 反思：在全局环境中安装```pandas```库，结果又发现没有安装```pip```，我才想起来我的很多库都是在虚拟环境.venv中安装的，于是提醒AI```是在虚拟环境中安装的，请在虚拟环境路径中调用```，然后AI成功安装并运行py脚本。 接着，我直接让AI删除所有在```全局```中和```虚拟环境```中重复安装过的包，或者删除```全局```中不必要的包，全部存入单独项目的```虚拟环境```中（这是个很大的坑！！！）


#### 2.使用DOI为论文PDF命名
> 步骤：导出PDF全文 → 查询元数据，找出对应DOI → 重命名文件

> 参考提示词：请处理 @文件夹 中的所有论文PDF文件。根据 @文件 中提供的DOI信息，将每个PDF复制一份，并将副本重命名为以下格式：
[DOI].pdf（DOI使用方括号[]包裹，保留PDF后缀）  
**要求：**  1. 确保每个PDF与对应的DOI正确匹配（基于@文件提供的元数据）。  2. 保留原始文件，不覆盖、不删除。  3. 若某篇论文在@文件中找不到DOI，请将该文件记录到一个清单（缺失清单），不要随意命名。

01 导出Zotero中引用数前20的论文，发现只有19篇有PDF，剩下一篇```Deep belief networks```在GenSpark也没找到，放弃。

02 根据 ```@文件 中提供的DOI信息，将每个PDF复制一份，并将副本重命名为以下格式。```这一步出现问题，因为没有采用前面整理的CSV文件，所以得**重新走一遍前面查找DOI的流程**。DOI补齐，然后开始rename，结果成功，19篇论文全部修改成功。

> 星宇老师演示出现一些问题，虽然我没有遇到，但是可以参照其思路，遇到问题逐一排查溯源。



#### 3.为补充类信息设置唯一编码

```
提示词：请处理 @sampled_tweets.csv 文件，将其中的每条 tweet 转换为独立的markdown(.md) 文件，并生成可追溯的索引。
具体要求如下：
1. **Markdown 文件生成**
    - 每条记录生成一个`.md`文件，内容仅包含该记录的`text`列内容。
    - 每个 .md 文件使用时间戳作为文件名，格式：YYYY-MM-DD-HH-MM-SS.md，例如 2025-07-11-14-15-34.md。
    - 确保文件名唯一，如遇重复时间戳，在文件名末尾追加序号 (如 -1, -2)。
2. **Excel 索引文件生成 (使用软链接)**
    - 在原始`CSV`基础上生成一个`Excel`文件`(.xlsx)`，保留`CSV`所有字段。
    - 新增一列`md_file`，记录该`tweet`对应 .md 文件的 相对路径软链接。
    - 建议 .md 文件和 tweets_with_md.xlsx 位于同一目录，以便 md_file 列可直接用文件名 (如 2025-07-11-14-15-34.md)，无需长路径。
    - 输出 Excel 文件命名为 tweets_with_md.xlsx。
3. **文件存储结构**
    - 新建目录 data/processed/tweets_md/，将所有 .md 文件及 tweets_with_md.xlsx 存放于该目录下。
    - 原始 sampled_tweets.csv 保持在 data/raw/ 目录，不得改动。
4. 最终输出结果需包括：
- 所有 .md 文件 (存于 data/processed/tweets_md/)
- tweets_with_md.xlsx (完整字段 + md_file 软链接列)
```

![补充类信息设置编码](https://s21.ax1x.com/2025/07/25/pVJKJOO.png)
- 起始文件：流程开始于一个蓝色的“csv文件”，该文件包含了“元数据+全文”信息。
- 文件转换：这个CSV文件经过一个“转换成”的操作，生成了一个粉色的“Md文件”（Markdown文件）。这个生成的Markdown文件会“使用时间戳命名”。
- 记录保存与链接：在转换的同时，原始的CSV文件会删除一些不需要的元数据，保存记录为一个橙色的新的“csv文件”（数据清洗）。
- 最后，橙色的“csv文件”通过“软链接”的方式（Excel某单元格超链接）指向新生成的Markdown文件，以便于追踪和关联。

导出Lisa F Barrett的其中30条推特数据为.csv格式，然后通过星宇老师的提示词运行。
第一次运行失败，产生了60个.md文件，询问AI原因，回答如下:
> **原因分析**：
因为在我们之前排查和解决Python环境问题的过程中，处理脚本被意外地执行了不止一次。第一次执行生成了30个文件，但由于时间戳完全相同，第二次执行时，脚本为了避免文件名重复，又创建了30个新的、带有递增序号的文件。这就导致了总共有60个文件。
**解决方案**：
为了确保结果的准确性，我将先彻底删除之前生成的 tweets_md 文件夹，然后重新运行一次脚本。这样就能保证我们得到一个干净、正确的结果，即30条数据对应30个 .md 文件。

第二次运行成功。

> 反思：还是出在**环境依赖**问题，原本我的安装的依赖都在**虚拟环境**中，但是AI运行时还是尝试在全局安装，看来得在**User Rules**里面提前备注，不然每次都要提醒，浪费时间。


### 二、去重
#### 1.Zotero自带去重功能
**Zotero自带重复条目筛选机制解析**

Zotero内置的“重复条目”(Duplicate Items)功能，其筛选机制相对比较保守和简单，主要是为了避免错误地将非重复的条目合并。其判断标准如下：

1. **优先匹配唯一标识符**：Zotero会首先检查**DOI (数字对象唯一标识符)** 和 **ISBN (国际标准书号)**。如果两条文献的DOI或ISBN完全相同，它们就会被立刻标记为重复。
2. **“模糊”匹配核心字段**：如果唯一标识符缺失或不匹配，Zotero会接着比较以下几个核心字段：
    - **标题 (Title)**
    - **作者 (Creator)**
    - **出版年份 (Year)**
    
    这里的匹配并非简单的“完全一致”。根据官方文档和社区讨论，其算法大致为：如果标题相似，并且至少有一位作者的姓氏和首字母缩写匹配，同时出版年份相近（通常在一年误差范围内），那么这两条文献就会被视为重复。
    

**为什么筛选不精准且不完全？**

了解了上述机制后，就很容易理解为什么您会觉得它“不精准”和“不完全”：

- **元数据质量不一**：很多从网页、数据库或PDF中抓取的文献，其元数据（Metadata）质量参差不齐。例如，标题中可能包含多余的标点、大小写不一致、或者附加了数据库来源信息；作者姓名可能有缩写和全名的差异；DOI字段可能缺失。这些细微的差别都会导致Zotero的算法无法识别它们为重复条目。
- **非唯一标识符的局限**：对于书籍的不同章节、或者同一会议录中的不同论文，它们可能共享相同的ISBN或书名，这会导致Zotero错误地将它们标记为“假性”重复（False Positives）。
- **算法保守**：为了安全起见，Zotero的查重算法宁可“漏杀一千”，也不愿“错杀一百”。只有在它非常有把握时，才会将条目放入“重复条目”列表。

#### 2.Zotero插件去重
目前最受推崇的是 **Zoplicate**。

**Zoplicate 插件的优势：**

- **灵活的查重标准**：您可以自定义用于比较的字段（如标题、作者、年份、期刊等），并调整其权重。
- **更高的查重精度**：通过更复杂的算法，能够识别出原生功能忽略的重复条目。
- **批量处理能力**：可以更方便地对检测出的重复条目进行批量合并，大大提高效率。
- **防止误合并**：提供了详细的对比界面，让您在合并前清楚地看到各项元数据的差异，自主决定保留哪个版本的信息。

#### 3.通用工具去重 dupeGuru
[dupeGuru](https://dupeguru.voltaicideas.net/) 是一款非常强大的重复文件检测工具，支持多种操作系统，包括 Windows、macOS 和 Linux。它不仅能够检测和删除重复文件，还可以对文件进行分类和整理。

1. **下载并安装 dupeGuru**：访问 dupeGuru 的官方网站，根据您的操作系统下载相应的安装包，并按照提示进行安装。
2. 启动 dupeGuru，“程序模式”选择“标准”。“扫描类型”选择“基于文件名比较”。其原理是通过MD5哈希值判断（MD5 (Message Digest Algorithm 5) 是**一种广泛使用的哈希函数，它将任意长度的数据转换为固定长度的128位哈希值，通常表示为32个十六进制字符**。MD5 具有不可逆性，即无法从哈希值反推出原始数据，因此常用于数据完整性校验和密码存储等场景。），如果两个文件的MD5哈希值相同，则认为它们是重复的。
3. “扫描类型”选择“基于内容比较”。其原理是通过比较文件内容来判断两个文件是否相同。如果两个文件的内容完全相同，则认为它们是重复的。
4. “程序模式”选择“图片”。原理是基于RGB值判断，如果两个图片的RGB值相同，则认为它们是重复的。

[imgdupes](https://github.com/knjcode/imgdupes) 是一个专门用于图片去重的工具，它使用的是一种基于内容的比较方法，通过比较图片的哈希值来判断图片是否重复。imgdupes 支持多种图片格式，包括 JPEG、PNG、GIF 等，并且可以处理大型的图片集。

1. **下载并安装 imgdupes**：
```markdown
# 安装依赖库
pip install imgdupes
# 搜索重复图片
imgdupes --recursive --phash 4 or 8
# 删除重复图片
imgdupes --recursive -d Images phash 4
# 删除重复图片（不询问用户）
imgdupes --recursive -d -N Images phash 4
```
> **踩了个坑**：首先我的pip装在虚拟环境.venv中，所以导致在全局环境下运行```pip install imgdupes```失败。然后我cd ～ .venv，运行安装，成功。
但是运行```imgdupes```失败了，提示”command not found: imgdupes”，询问AI后，给了一些方案，我最终采取了“方案1：每次使用前激活虚拟环境（推荐）”

```markdown
# 激活虚拟环境
source /Users/tommy/Projects/.venv/bin/activate
# 使用 imgdupes
imgdupes /path/to/images phash 10 -r
# 退出虚拟环境（可选）
deactivate
```

```markdown
# 基础使用：递归搜索重复图片
imgdupes ~/Pictures phash 4 -r

# 安全模式：先预览再决定
imgdupes ~/Pictures phash 4 -r -m --dry-run

# 交互删除：手动选择保留哪些文件
imgdupes ~/Pictures phash 4 -r -d

# 高性能：使用多进程和缓存
imgdupes ~/Pictures phash 4 -r --num-proc 8
```

#### 4.语义查重工具 FastGPT
[FastGPT](https://cloud.fastgpt.cn/login)是一个基于 LLM 大语言模型的知识库问答系统，将智能对话与可视化编排完美结合，让 AI 应用开发变得简单自然。无论您是开发者还是业务人员，都能轻松打造专属的 AI 应用。

- 首先利用FastGPT给照片查重，通过导入图片库，利用Qwen-vl-max模型查重，复刻成功。

- 然后通过利用文本向量化，建立Lisa推特数据的本地知识库，通过语义查找Lisa的推特文本，复刻成功。

#### 5.总结
| 去重工具    | 功能            |
|-------------------------------|--------------------------------|
| Zotero自带去重功能     | 元数据去重                |
| Zotero去重插件Zoplicate       | 元数据去重        |
| dupeGuru       | 元数据去重、相同内容的文件去重、图片去重     |
| imgdupes       | 图片去重         |
| FastGPT                       | 相似图片、文本查询                           |

### 三、分组
#### 1.将论文按照时间分组
根据提示词
```markdown
请处理 @文件夹 中的所有 PDF 文件，结合 @csv 中的元数据，根据论文的年份对 PDF 文件进行分类整理。

具体要求如下：

1. **基于年份分类整理**  
   - 使用 CSV 文件中的元数据（假设包含论文文件名及年份字段），为每个 PDF 找到对应年份。  
   - 按年份将 PDF 文件分别复制到以年份命名的子文件夹中，例如：  
     `data/processed/organized_papers_by_year/2015/`、`data/processed/organized_papers_by_year/2016/` 等。  

2. **复制而非移动**  
   - 保留原始 `@/renamed_papers` 目录中的文件不变，只在新目录中生成分类结果。  
   - 如有 PDF 未在元数据中找到匹配年份，请统一放入 `data/processed/organized_papers_by_year/unknown/` 文件夹。  

3. **输出目录结构**  
   - 所有整理后的文件放在 `data/processed/organized_papers_by_year/` 下。  
   - 目录结构示例：  
     ```
     data/processed/organized_papers_by_year/
       ├─ 2015/
       ├─ 2016/
       ├─ 2017/
       └─ unknown/
     ```

最终输出需包括：  
- 按年份分类的 PDF 副本（保留原始文件不变）  
`data/processed/organized_papers_by_year/` 完整目录结构
修改日志
```

按照提示词，成功复刻。


#### 2.将论文按照空间分组
提示词
```markdown
请根据提供的 @csv文件 ，查询其中所有论文作者中名为“Trevor Robbins”的教授所属机构信息，并输出一个新的CSV文件。  

具体要求：  
1. 使用 OpenAlex API 查询，获取 Trevor Robbins 在每篇论文中的所有关联机构（可能不止一个）。  
2. 输出结果为新的 excel 文件，列包含以下字段：  
   - 作者（Author）  
   - 标题（Title）  
   - DOI  
   - 发表日期（Publication Date）  
   - 机构（Affiliations，多个机构用“|”分隔）  
   - PDF 软链接（PdfPath）：指向 `` 目录下的对应 PDF 文件，可以使用excel软链接功能点击直接打开。  
3. PDF 文件已使用 DOI 命名，例如：`[10.1007_s00213-002-1154-7].pdf`。  
4. 确保输出文件中每条记录对应原始论文信息，并完整列出 Trevor Robbins 的全部关联机构。

最终输出为处理后的 excel 文件。
```
复刻这一步卡了一段时间，主要几个方面，一是运行py脚本过程中出现了问题，软链接无法对应PDF文件名，重复尝试两次都不行，怀疑是模型问题，家里电脑梯子不行，用的Claude 4 sonnet，估计没有gemini-2.5-pro好用。

#### 3.将论文按照研究主题分组
提示词
```markdown
请你在这个文件 /Users/tommy/Projects/AI学术分析/智能整理/data/processed/Trevor_Robbins_PDF机构信息_20250726_220204.xlsx文件 的基础上，获取每个条目的openalex topics。
你可以查询@202507261928-Trevor-Robbins-DOI-20.csv 进行获取。
新增一列“论文主题”，用于填写上述找到的topics，
保存为 data/processed/Trevor-Robbins_with_Affiliations_v3.xlsx。

注意：
1、如果通过doi无法匹配，可以使用标题匹配两个表格

2、excel表格中的pdfpath是文件软链接，新的excel一定要保留，使得可以点击。
```

按照提示词复刻成功。

### 四、完善元数据

#### 1.获取某学术数据库快速检索的方法
```markdown
我有一个文件：@csv文件，里面包含 30 篇论文的题录信息（含 DOI）。  
请问，有没有快速的方法能把这些 DOI 或其他元数据批量导入 Web of Science (WOS)，
以便一次性检索并获取这些论文的完整记录？ 
```
提示词复刻成功，WOS需要TB上特殊渠道进去，才能使用高级功能。导出WOS上的数据，发现没有keywords。

```markdown
#### 2.合并不同来源的元数据
请将 @xlsx文件 中的 “Author Keywords” 和 “Keywords Plus” 两列，  
合并到 @xlsx文件 中，  
新列分别命名为 author_keywords_wos 和 keywords_plus_wos。  
匹配时，请基于 DOI 精确匹配；若 DOI 缺失，则基于标题相似度（模糊匹配）对齐记录。  
完成后，生成一个包含所有原始列及新增两列的新的 Excel 文件。
```
复刻成功。

#### 3.给md文件加上元数据标签
```markdown
请将一个CSV文件@sampled_tweets.csv 中的每一行内容，转换为一个独立的Markdown（.md）文档。  
每个文档需要在开头生成以下元标签（YAML Front Matter），并根据CSV数据和规则补全内容：

---
title:    {根据时间列转换为中文日期，例如 "二零二零年三月二十一日 二十三点二分"}
excerpt:  "{从CSV中的question字段提取前N字作为摘要，自动加双引号}"
layout:   defult   # 固定为 defult
category: {CSV中type列的值}
keywords: [{使用KeyBERT从正文中提取关键词列表}]
source:   [X@LFeldmanBarrett]   # 固定不变
uin:      {根据时间列生成唯一编码，例如20200321230229}
---

要求：
1. 从CSV中的相应字段提取数据（如type、时间、正文等）。
2. 时间需转换为中文大写数字日期，并补充具体时间（小时、分钟）。
3. excerpt 自动截取指定长度（如前50字），加上双引号。
4. keywords 使用KeyBERT对正文提取，结果为数组格式。
5. 每行CSV生成一个单独的.md文件，文件名建议使用uin值。
6. 输出结果保持Markdown格式。
```

我选的Trevor Robbins没有推特数据，于是借用课程的lisa数据，复刻成功。